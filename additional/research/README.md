# Создание CNN для распознавания сгенерированных изображений

## Цель

Цель исследования обучить модель распознавать сгенерированные изображения.

## Результаты

### Первые результаты

Для первого обучения были выбраны такие гиперпараметры:

|Параметр|Значение|
|--------|--------|
|Количество conv блоков|4|
|Количество слоев классификатора|2|
|Размер датасета|1000|
|train / val|0.5|
|Эпох|30|

Результат:

|Результат|Значение|
|---------|--------|
|Train Accuracy|0.64|
|Validation Accuracy|0.62|

Точность на обучении и валидации остается ниже ```65%```, что близко к случайному угадыванию (```50%```). Присутствует нестабильность, особенно на валидации, возможно, переобучение на шум или нерегулярные признаки. Также модель не переобучается сильно, но и не обучается хорошо.

Анализируя Confusion Matrix, можно сказать, что настоящие изображения распознается лучше, чем сгенерированные. Модель очень часто принимает сгенерированное изображение за реальное (126 раз предсказано реальное, но являлось сгенерированным)

Модель пока не уверенно распознает сгенерированные изображения.

На следующем цикле обучения стоит попробовать увеличить датасет (1000 изображений, которые использовались в этом цикле, довольно мало). Также попробовать добавить больше conv блоков, слоев классификатора и увеличить число базовых каналов до 128 (кажется, что у модели мало параметров и она довольно узкая чтобы распознавать сгенерированные изображения).

### Вторая итерация

Гиперпараметры:

|Параметр|Значение|
|--------|--------|
|Количество conv блоков|6|
|Количество слоев классификатора|2|
|Размер датасета|5000|
|train / val|0.7|
|Эпох|30|

Результат:

|Результат|Значение|
|---------|--------|
|Train Accuracy|0.50|
|Validation Accuracy|0.50|

Получился ужасный результат, модель абсолютно случайно делает предсказания и не обучается вообще. Проблема скорее всего в том что модель слишком большая для малого датасета.

Confusion Matrix выглядят очень странно, особенно на валидации ```1500 / 1500``` раз модель предсказала изображение как сгенерированное. Почему так произошло - не понятно, возможно, это также связано с тем что размер модели и кол-во данных подобраны не верно.

В этом исследование [DeepGuardNet: A Novel CNN Architecture for DeepFake Image Detection](https://www.sciencedirect.com/science/article/pii/S1877050925014152) обучали модель с очень похожей на мою архитектуру, но заточенную под обнаружение DeepFake в нем они использовали ```140002``` изображений для обучения модели из 5 conv блоков, в каждом из которых не больше 16 каналов + 1 слой классификатора.

Также в этом исследование [Detection of AI-Generated Synthetic Images with a Lightweight CNN](https://www.mdpi.com/2673-2688/5/3/76) использовали модель архитектурно очень похожую на мою и обучали ее на ```320000``` изображений.

Прошлое предположение:

> Также попробовать добавить больше conv блоков, слоев классификатора и увеличить число базовых каналов до 128

Было неверным, нужно оставить предыдущую архитектуру и увеличить кол-во данных. В используемом датасете есть 2.5 млн картинок около 900 тыс настоящие, остальные - сгенерированные ИИ. Для следующего обучения стоит попробовать взять хотя бы 200 000 изображений.

### Третья итерация

Гиперпараметры:

|Параметр|Значение|
|--------|--------|
|Количество conv блоков|4|
|Количество слоев классификатора|2|
|Размер датасета|200000|
|train / val|0.7|
|Эпох|17|

Результат:

|Результат|Значение|
|---------|--------|
|Train Accuracy|0.63|
|Validation Accuracy|0.64|

Получился снова плохой результат. Проанализируем графики:

- Accuracy застыла около 0.64
- Train/Val accuracy практически идентичны
- Train loss стабилизировался на 0.64
- Validation loss нестабилен

Это значит, что модель не переобучается, но и не обучается дальше определенного уровня. Скачки Validation loss скорее всего связаны с тем что модель считает одно изображение уверено сгенерированным, потом чуть менее уверенно и теряет уверенность на валидации.

Из Confusion Matrix видно что модель склонна ошибаться чаще на generated, принимая их за реальные.

Модель даже не распознает явно сгенерированное лицо, хотя это очевидный DeepFake. Значит, она фокусируется не на глобальных чертах, а на каких-то текстурных паттернах, и все интерпретирует как real, если не нашла очевидных признаков подделки.

Модель уперлась в потолок возможностей своей архитектуры: она стабильно предсказывает реальное, даже на подделках, точность и потери замерли.

Попробуем ResNet18 на том же датасете:

```python
from torchvision.models import resnet18, ResNet18_Weights
from torch import nn
from torch.utils.data import DataLoader

model = resnet18(weights=ResNet18_Weights.DEFAULT).to('cuda')
model.fc = nn.Identity()
model.eval()

features, labels = [], []
for x, y in DataLoader(dataset.train_dataset, batch_size=64):
    with torch.no_grad():
        f = model(x.to("cuda")).cpu()
    features.append(f)
    labels.append(y)

X = torch.cat(features).numpy()
y = torch.cat(labels).numpy()

from sklearn.linear_model import LogisticRegression
clf = LogisticRegression(max_iter=1000).fit(X, y)
print("LogReg accuracy:", clf.score(X, y))
```

Получаем accuracy 0.7 и roc-auc 0.77 обучая всего около 10 минут на датасете из 2000 картинок. Значит, что в данных есть различия между real и generated, которые улавливаются на высокоуровневых признаках и моя модель не обучается именно из-за того что не учится отличать глобальные черты, а смотрит на локальные текстуры.

Поэтому если не выходит обучить свою модель, то следует обучить ResNet18 и использовать ее.

### Четвертая итерация

Гиперпараметры:

|Параметр|Значение|
|--------|--------|
|Модель|ResNet18|
|Размер датасета|200000|
|train / val|0.7|
|Эпох|21|

Результат:

|Результат|Значение|
|---------|--------|
|Train Accuracy|0.65|
|Validation Accuracy|0.66|
|Train Loss|0.63|
|Validation Loss|0.62|

Получился снова не лучший результат. Использовалась модель ResNet18 с измененным последним слоем на Linear(512, 1) и получилось качество только 0.65-0.66.

Анализ графиков

- Скачки на валидации
- Train accuracy стабилен
- Confusion matrix: high FN и FP

Потому что train accuracy стабилен, можно сделать вывод что модель хорошо запоминает, но не обобщает, также об этом говорит поведение Validation Accuracy и Validation Loss.

В этой итерации модель обучалась полностью: обучались все слои. Судя по небольшому тесту с прошлой итерации (когда было обучение логистической регрессии) обучать все слои - ошибка, стоит попробовать немного увеличить последний слой классификатора и обучить его, а затем прогнать несколько эпох обучая всю модель.
