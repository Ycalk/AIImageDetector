# Создание CNN для распознавания сгенерированных изображений

## Цель

Целью данного исследования является разработка и обучение модели глубокого обучения, способной эффективно различать сгенерированные ИИ-изображения и реальные фотографии, а также анализ влияния архитектуры модели, объема данных и стратегии обучения на итоговую точность и устойчивость классификации. Особое внимание уделяется обучению более простых моделей для последующего использования на CPU.

## Результаты

### Примечание

Обучение моделей производилось на Tesla T4 [в этом ноутбуке](https://colab.research.google.com/drive/1aSdn4Ii2Ee0a41l-bR15rzUTnaGvmuvm?usp=sharing). Для обучения использовались PyTorch, torchvision и sklearn. Результаты представлены в этом файле, а также в папке results. Скачать модели можно [здесь](https://huggingface.co/ycalk/AIImageDetector).

### Первые результаты

Для первого обучения были выбраны такие гиперпараметры:

|Параметр|Значение|
|--------|--------|
|Количество conv блоков|4|
|Количество слоев классификатора|2|
|Размер датасета|1000|
|train / val|0.5|
|Эпох|30|

Результат:

|Результат|Значение|
|---------|--------|
|Train Accuracy|0.64|
|Validation Accuracy|0.62|

Точность на обучении и валидации остается ниже ```65%```, что близко к случайному угадыванию (```50%```). Присутствует нестабильность, особенно на валидации, возможно, переобучение на шум или нерегулярные признаки. Также модель не переобучается сильно, но и не обучается хорошо.

Анализируя Confusion Matrix, можно сказать, что настоящие изображения распознаются лучше, чем изображения сгенерированные ИИ. Модель очень часто принимает сгенерированное изображение за реальное (126 раз изображение было предсказано как реальное, но на самом деле оказалось сгенерированным).

Модель пока не уверенно распознает сгенерированные изображения.

На следующем цикле обучения стоит попробовать увеличить датасет (1000 изображений, которые использовались в этом цикле, довольно мало). Также попробовать добавить больше conv блоков, слоев классификатора и увеличить число базовых каналов до 128 (кажется, что у модели мало параметров и она довольно узкая чтобы распознавать сгенерированные изображения).

### Вторая итерация

Гиперпараметры:

|Параметр|Значение|
|--------|--------|
|Количество conv блоков|6|
|Количество слоев классификатора|2|
|Размер датасета|5000|
|train / val|0.7|
|Эпох|30|

Результат:

|Результат|Значение|
|---------|--------|
|Train Accuracy|0.50|
|Validation Accuracy|0.50|

Результат оказался крайне плохим, модель делает предсказания абсолютно случайным образом и не обучается вообще. Проблема скорее всего в том, что модель слишком большая для малого датасета.

Confusion Matrix выглядят очень странно, особенно на валидации ```1500 / 1500``` раз модель предсказала изображение как сгенерированное. Причины такого поведения модели неясны, возможно, это также связано с тем что размер модели и количество данных подобраны неверно.

В этом исследовании [DeepGuardNet: A Novel CNN Architecture for DeepFake Image Detection](https://www.sciencedirect.com/science/article/pii/S1877050925014152) обучали модель с очень похожей на мою архитектуру, но заточенную под обнаружение DeepFake в нем они использовали ```140002``` изображений для обучения модели из 5 conv блоков, в каждом из которых не больше 16 каналов + 1 слой классификатора.

Также в этом исследовании [Detection of AI-Generated Synthetic Images with a Lightweight CNN](https://www.mdpi.com/2673-2688/5/3/76) использовали модель архитектурно очень похожую на мою и обучали ее на ```320000``` изображений.

Прошлое предположение:

> Также попробовать добавить больше conv блоков, слоев классификатора и увеличить число базовых каналов до 128

Было неверным, нужно оставить предыдущую архитектуру и увеличить кол-во данных. В используемом датасете есть 2.5 млн картинок около 900 тыс. - настоящие, остальные - сгенерированные ИИ. Для следующего обучения стоит попробовать взять хотя бы 200 000 изображений.

### Третья итерация

Гиперпараметры:

|Параметр|Значение|
|--------|--------|
|Количество conv блоков|4|
|Количество слоев классификатора|2|
|Размер датасета|200000|
|train / val|0.7|
|Эпох|17|

Результат:

|Результат|Значение|
|---------|--------|
|Train Accuracy|0.63|
|Validation Accuracy|0.64|

Получился снова плохой результат. Проанализируем графики:

- Accuracy застыла около 0.64
- Train/Val accuracy практически идентичны
- Train loss стабилизировался на 0.64
- Validation loss нестабилен

Это значит, что модель не переобучается, но и не обучается дальше определенного уровня. Скачки Validation loss скорее всего связаны с тем что модель сначала уверенно считает изображение сгенерированным, затем менее уверенно, теряя уверенность на валидации.

Из Confusion Matrix видно что модель склонна ошибаться чаще на generated, принимая их за реальные.

Модель даже не распознает явно сгенерированное лицо, хотя это очевидный DeepFake. Значит, она фокусируется не на глобальных чертах, а на каких-то текстурных паттернах, и все интерпретирует как real, если не обнаружила признаков подделки.

Модель уперлась в потолок возможностей своей архитектуры: она стабильно предсказывает реальное, даже на подделках, точность и потери замерли.

Попробуем ResNet18 на том же датасете:

```python
from torchvision.models import resnet18, ResNet18_Weights
from torch import nn
from torch.utils.data import DataLoader

model = resnet18(weights=ResNet18_Weights.DEFAULT).to('cuda')
model.fc = nn.Identity()
model.eval()

features, labels = [], []
for x, y in DataLoader(dataset.train_dataset, batch_size=64):
    with torch.no_grad():
        f = model(x.to("cuda")).cpu()
    features.append(f)
    labels.append(y)

X = torch.cat(features).numpy()
y = torch.cat(labels).numpy()

from sklearn.linear_model import LogisticRegression
clf = LogisticRegression(max_iter=1000).fit(X, y)
print("LogReg accuracy:", clf.score(X, y))
```

Получаем accuracy 0.7 и roc-auc 0.77 обучая всего около 10 минут на датасете из 2000 картинок. Значит, что в данных есть различия между real и generated, которые улавливаются на высокоуровневых признаках и моя модель не обучается именно из-за того что не учится отличать глобальные черты, а смотрит на локальные текстуры.

Поэтому если не выходит обучить свою модель, то следует обучить ResNet18 и использовать ее.

### Четвертая итерация

Гиперпараметры:

|Параметр|Значение|
|--------|--------|
|Модель|ResNet18|
|Размер датасета|200000|
|train / val|0.7|
|Эпох|21|

Результат:

|Результат|Значение|
|---------|--------|
|Train Accuracy|0.65|
|Validation Accuracy|0.66|
|Train Loss|0.63|
|Validation Loss|0.62|

Получился снова не лучший результат. Использовалась модель ResNet18 с измененным последним слоем на ```Linear(512, 1)``` и точность составила лишь ```0.65``` - ```0.66```.

Анализ графиков

- Скачки на валидации
- Train accuracy стабилен
- Confusion matrix: high FN и FP

Потому что train accuracy стабилен, можно сделать вывод что модель хорошо запоминает, но не обобщает, также об этом говорит поведение Validation Accuracy и Validation Loss.

В этой итерации модель обучалась полностью: обучались все слои. Анализ предыдущей итерации (когда было обучение логистической регрессии) показал, что обучать все слои - ошибка, стоит попробовать немного увеличить последний слой классификатора и обучить его, а затем прогнать несколько эпох обучая всю модель.

### Финальная итерация

Гиперпараметры:

|Параметр|Значение|
|--------|--------|
|Модель|ResNet18|
|freeze features|True|
|Размер датасета|200000|
|train / val|0.7|
|Эпох|20|

Результат:

|Результат|Значение|
|---------|--------|
|Train Accuracy|0.71|
|Validation Accuracy|0.72|
|Train Loss|0.55|
|Validation Loss|0.53|
|Время инференса|0.458|

Анализ графиков:

- Тренд Accuracy
  - Постепенный рост на обеих выборках
  - Validation accuracy > train accuracy — модель не переобучается, а, наоборот, немного недообучена
- Тренд Loss
  - Train Loss постепенно снижается и стабилизируется
  - Validation Loss снижается, но начинает колебаться ближе к концу

Модель обучается стабильно. Заморозка весов помогла избежать переобучения и добиться улучшения на валидации.

Анализ Confusion Matrix

Модель предпочитает класс "реальный", ошибаясь в пользу реальности даже при наличии фейка. Это может быть связано осторожностью модели.

Анализ Grad-CAM

Для сгенерированного изображения внимание сосредоточено на лице, это информативно и ожидаемо. Для реального изображения активации более размытые, охватывают весь объект, но также фокус есть. Значит, что модель ориентируется на высокоуровневые черты, но, вероятно, признаки недостаточно выразительны для уверенного различения.

## Итоги

|Категория|Описание|
|---------|--------|
|Модель|ResNet18 с замороженными весами, обучается только голова|
|Датасет|200 тыс. изображений, 50/50 real/fake, 70/30 train/val|
|Финальный результат|```0.72``` accuracy на валидации, ```0.53``` loss|
|Поведение|Стабильное обучение, отсутствует переобучение|
|Направление внимания|Модель ищет глобальные черты, Grad-CAM визуализация осмысленная|

## Вывод

Был пройден путь от кастомной архитектуры до ResNet18 с анализом метрик, логической интерпретацией и визуализацией внимания. Модель с замороженной ResNet18 и дообученной головой дала наилучший результат, и, что важно очень устойчивый.

Небольшие выводы по всему исследованию в целом:

- Глубокие признаки важнее локальных паттернов. Кастомная CNN фокусировалась на текстурах, что не позволило ей обобщать. ResNet18, напротив, способна улавливать глобальные различия между real и fake, что подтверждают визуализации Grad-CAM и стабильные метрики
- Заморозка features оказалась эффективнее полного обучения. Полное обучение всех слоев приводило к переобучению на шум, в то время как обучение только головы дало лучшее качество и стабильность
- Модель демонстрирует поведение, склоняясь к предсказанию реальности, даже в случае подделок. Это делает её надежной в высокочувствительных сценариях

В дальнейшем можно развивать подход, применяя продвинутые архитектуры, например, Vision Transformer или фокусироваться на специфических аномалиях генерации конкретных моделей.
